.. _G53MLE07:

====================
07 - Data Clustering
====================

So far we have used so-called **supervised learning** where for a given input
there is a known output, with which the machine can adjust it's parameters.
This is the case with Neural Networks, K-Nearest Neighbour and Bayesian
learning.

Sometimes, however, we have data but we don't know how that data is classified.
For example, if we wish to discover patterns or intrinsic relationships that
exist in the data set - to learn more about that data or use it in an
appropriate way.

This sort of application requires **unsupervised learning**, and is often
referred to as *Data Mining*. Data Clustering is a technique for achieving this
goal.

Example - GIF Compression
-------------------------

To compress an image with the GIF format, a colour table is used with (for
example) 256 colours. Each coloured pixel in the original image is compared
with the colour table, to find the closest colour. The index of this colour is
then recorded in the GIF image.

To find the closest colour, the original colours (R, G, B) are projected onto
a 3-dimensional feature space, and then "clouds" of similar colours are
grouped, and replaced with one colour in the GIF colour table.

K-Means
-------

K-Means is an algorithm for partitioning (clustering) :math:`N` data points into
:math:`K` disjoint subsets :math:`S_j` containing  :math:`N_j` data points.

The algorithm defines a cluster "centroid" or "prototype" for each cluster,
which can be randomly chosen at the starting point.

When a new sample is added, the Euclidean distance between it and the existing
prototypes is calculated, and it is assigned to the cluster whose prototype is
closest.

The Algorithm
^^^^^^^^^^^^^

**Step 1**

Arbitrarily choose from the given sample set :math:`k` initial cluster centres.
For example, the first :math:`k` samples could be used, or they could be chosen
at random.

Set :math:`t = 0` (the iteration index).

**Step 2**

Assign each of the samples to one of the clusters according to the distance
between the sample and the centre of the cluster.

.. math::

    X(i) \in C^{(t)}(j)

    if~D(X(i), M^{(t)}(j)) \leq D(X(i), M^{(t)}(l))

    for~all~l = 1, 2, ..., k

**Step 3**

Update the cluster centres to be the average of all the points in the
corresponding cluster.

.. math::

    M^{(t+1)}(j) = \frac{1}{N^{(t)}_j} \sum_{X(i) \in C^{(t)}(j)} X(i)

**Step 4**

Calculate the error of approximation, to monitor the progress of the algorithm.

.. math::

    E(t) = \frac{1}{2} \sum_{j=1}^K \sum_{X(i) \in C^{(t)}(j)}
        ||X(i) - M^{(t)}(j)||^2

This quantity is expected to reduce as the algorithm progresses.

**Step 5**

If the terminating criterion is met, then terminate.

Otherwise, set :math:`t = t+1` and go to step 2.

Stopping Criteria
^^^^^^^^^^^^^^^^^

The following stopping criteria can be used for the K-Means algorithm:

#. The errors do not change significantly in two consecutive epochs:
    :math:`|E(t) - E(t-1)| < \epsilon` where :math:`\epsilon` is some small,
    preset value
#. No further change in the assignment of the data points to clusters in two
   consecutive epochs.
#. After a fixed number of epochs.

Remarks
^^^^^^^

* K-Means is a gradient descent algorithm, which attempts to minimize a cost
  function :math:`E`.
* In general, the algorithm does not achieve a global minimum of :math:`E` over
  the assignments.
* The algorithm is sensitive to the initial choice of cluster centres. Different
  starting cluster centroids may lead to a different solution.
* It is a popular method, and many more advanced methods have been derived from
  this simple algorithm.
