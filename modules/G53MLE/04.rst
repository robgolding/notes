.. _G53MLE04:

===========================
04 - Multilayer Perceptrons
===========================

Limitations of Single Layer Perceptrons
---------------------------------------

A single layer perceptron can only express a linear decision surface. To
overcome this limitation, we must build a multilayer network, which is able to
represent a highly non-linear decision surface.

Multilayer Networks
-------------------

Multilayer networks consist of an *input* layer, a *hidden* layer, and an
*output* layer. Each layer consists of a number of *units*.

Each unit has a number of weighted input (like a Perceptron). These weighed
inputs are summed in the same way, but the sum is then passed through an
*activation function* - which is a non-linear *sigmoid* function that ranges
from :math:`0 - 1` - before being output.

The input units in a network such as this do not use an activation function.
They simply distribute (or "fan out") their input directly to the units in the
next layer.

Sigmoid Function
----------------

:math:`\sigma(x)` is the sigmoid function:

.. math::

    \frac{1}{1+e^{-x}}

This function has the useful property that it's derivative is equal to:

.. math::

    \frac{\delta \sigma(x)}{\delta x} = \sigma(x)(1 - \sigma(x))

We can therefore use gradient descent to train the network.

Back-Propagation Algorithm
--------------------------

.. math::

    E(W) = \frac{1}{2}\sum_{k=1}^{K}(d(k) - y(k))^2
    
    \frac{\delta E}{\delta w_i} = -\sum_{k=1}^{K}((d(k)
    - y(k))y(k)(1-y(k))x_i(k))

Expressive Capabilities
-----------------------

Every boolean function can be represented by a network with a single hidden
layer (though it might require an exponential number of hidden units).

Every bounded continuous function can be approximated with arbitrarily small
error, by a network with one hidden layer.

Any function can be approximated to arbitrary accuracy by a network with two
hidden layers.

Overfitting, Generalization and Stopping Criterion
--------------------------------------------------

We can continue training until the error ``E`` falls below some predefined
value.

This is not a good idea, however, as backpropagation has a tendency to
*overfit* the training examples, at the cost of decreasing generalization
accuracy over the unseen examples.

Alternatively, we can use a *validation set*, along side the training set. This
validation set is used to test the network on unseen data, so the training
process can be stopped when the error over the validation set falls below
a predefined value.
