.. _G53MLE08:

=======================================
08 - Principal Component Analysis (PCA)
=======================================

In most cases, the raw data we collect from sensors (cameras, etc.) is very
highly dimensional (i.e. 100s of dimensions). Generally, we use *feature
extraction* to reduce the dimensionality of this raw data, whilst still
retaining the useful information - before passing the data to the classifier.

The basic principle of feature extraction is to transform a raw data vector
:math:`X` (of :math:`N` dimensions) to a new vector :math:`Y` (of :math:`n`
dimensions), where :math:`n << N`, via a transformation matrix :math:`A` such
that :math:`Y` captures most of the information in :math:`X`.

.. math::

    Y = AX

Principle Component Analysis
----------------------------

Principle Component Analysis is one of the most common dimensionality reduction
techniques.

The first principle component is the "direction" by which the data varies the
largest. Subsequent principle components are orthogonal to the first PC, and
describe the maximum residual variance.

PCA has many applications, including:

* Visualisation
* Data reduction
* Data classification
* Trend analysis
* Factor analysis
* Noise reduction

Redundancy
----------

If, for example, two pieces of data are recorded (:math:`r1` and :math:`r2`).
If the two values are uncorrelated (so the graph looks random, possibly
circular), then there is no redundancy between the two recordings.

Alternatively, if they are highly correlated, then knowing the value of one
item, we can predict the value of the other. Therefore there is high redundancy
in the data.

Covariance Matrix
-----------------

The covariance matrix describes all relationships between pairs of measurements
in our data set.

A larger covariance indicates large correlation (so lots of redundancy), and
zero covariance indicates no correlation (so no redundancy).

If our goal is to reduce redundancy, then we want each variable to co-vary as
little as possible. Precisely, we want the covariance between separate
measurements to be zero.

Hpw it Works
------------

* PCA first selects a normalised direction in m-dimensional space along which
  the variance of X is maximised - it saves this direction as :math:`p_1`
* It then finds another direction, along which variance is maximised subject to
  the orthonormal condition - it restricts its search to all directions
  perpendicular to all previous selected directions
* The process can continue until m directions are found. The resulting ordered
  set of p's are the **principal components**
* The variances associated with each direction :math:`p_i` quantify how
  principal (important) each direction is - thus rank-ordering each basis
  according to the corresponding variance

PCA Procedures
--------------

First get the data (of course).

#. Subtract the mean from each data point (in each dimension separately)
#. Calculate the covariance matrix
#. Calculate the eigenvectors and eigenvalues of the covariance matrix
